# Github_Issue_Summarizer
ğŸª´ AI-Powered GitHub Issue Assistant

An intelligent tool that reads any GitHub issue and automatically classifies it into a structured JSON summary â€” identifying the issue type, priority, labels, and potential impact â€” using Googleâ€™s Gemini API and live GitHub data.

ğŸ¯ Built for the Seedling Labs Engineering Craft Case
Focus: AI-assisted problem solving, clean system design, and thoughtful engineering.

ğŸŒ Table of Contents

Overview

Architecture

Prompt Engineering Strategy

Edge Cases Handled

Setup & Installation

Running the App

Usage Demo

Testing

Performance & Speed

Going the Extra Mile

Future Enhancements

ğŸ§  Overview

The AI-Powered GitHub Issue Assistant analyzes issues directly from any public GitHub repository and produces a compact, well-structured JSON output.

Each analysis includes:

{
  "summary": "Short plain-English summary",
  "type": "bug | feature_request | documentation | question | other",
  "priority_score": "N - brief justification",
  "suggested_labels": ["label1", "label2"],
  "potential_impact": "Concise impact statement"
}


It uses:

FastAPI for an asynchronous backend

httpx for GitHub API calls

Google Gemini for structured summarization

Streamlit for a clean, interactive frontend

ğŸ§© Architecture
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Streamlit UI          â”‚
â”‚ (repo URL + issue number)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ HTTP POST /analyze
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          FastAPI              â”‚
â”‚  1. Validate input            â”‚
â”‚  2. Fetch issue + comments    â”‚
â”‚  3. Build LLM prompt          â”‚
â”‚  4. Call Gemini (JSON output) â”‚
â”‚  5. Validate w/ Pydantic      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Streamlit UI           â”‚
â”‚  â€¢ Summary                    â”‚
â”‚  â€¢ Type / Priority / Labels   â”‚
â”‚  â€¢ Downloadable JSON          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


âœ… Tech Stack

Language: Python 3.11

Backend: FastAPI + httpx + Pydantic

Frontend: Streamlit

AI: Google Gemini 1.5 (via google-generativeai)

Hosting Ready: Hugging Face Spaces / Render / Deta (free tiers)

ğŸ¯ Prompt Engineering Strategy

Prompting accounts for 40 % of the evaluation â€” hereâ€™s how itâ€™s optimized:

1. Strict Schema Enforcement

Gemini is instructed to produce JSON only, validated by Pydantic.

2. Multi-Shot Few-Shot Prompting

Includes 4 examples (Bug, Feature Request, Documentation, Question) to improve structure and consistency.

3. Error Recovery

If JSON parsing fails, the backend can retry with a â€œrepair prompt.â€

4. Contextual Guidance

Prompts specify output constraints, type hints, and multilingual handling (auto-translate to English if needed).

5. Truncation & Summarization

Long issue bodies/comments are truncated (~8 k chars) to keep inference efficient.

âš™ï¸ Edge Cases Handled
Edge Case	Behavior
âŒ Invalid repo URL	Returns 400 with helpful hint
ğŸ”’ Private repo / no token	403 with message to set GITHUB_TOKEN
ğŸš« Issues disabled	Early rejection with explicit message
â“ No comments or body	Still analyzed using title/context
ğŸ§¾ Very long body/comments	Truncated to prevent token overflow
ğŸŒ Non-English text	Translated mentally; JSON output in English
â³ Rate limit	Friendly â€œRate limitedâ€ warning
ğŸ§± Invalid model name	Fallback to working Gemini model
âš¡ Setup & Installation
ğŸ§° Prerequisites

Python 3.11 (Recommended)

A Google AI Studio API Key (starts with AIza)

(Optional) GitHub token for higher API limits

1ï¸âƒ£ Clone & Create venv
git clone https://github.com/<your-username>/ai-github-issue-assistant.git
cd ai-github-issue-assistant
python -m venv venv
venv\Scripts\activate    # Windows
# source venv/bin/activate  # macOS/Linux

2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

3ï¸âƒ£ Configure Environment Variables

Create backend/.env:

GOOGLE_API_KEY=AIza...your_key_here
GITHUB_TOKEN=ghp_...optional_token
MODEL_NAME=models/gemini-1.5-flash-latest

ğŸƒ Running the App
â–¶ Run Backend
cd backend
uvicorn main:app --host 0.0.0.0 --port 8000 --reload


Visit â†’ http://127.0.0.1:8000/docs

â–¶ Run Frontend
cd ../frontend
streamlit run streamlit_app.py


Visit â†’ http://localhost:8501

ğŸ’¡ Usage Demo

Enter a repo URL (e.g. https://github.com/facebook/react)

Enter an issue number (e.g. 27000)

Click â€œAnalyze Issueâ€

View the classification with:

Summary

Type

Priority

Suggested Labels

Potential Impact

Downloadable JSON

ğŸ§ª Testing

Run with pytest:

pip install pytest
pytest -q


Includes tests for:

/healthz

Invalid repo URLs

Missing comments/bodies

Stubbed Gemini responses

Priority format & label count validation

âš™ï¸ Performance & Speed
Optimization	Description
âš¡ Async httpx	Parallel GitHub calls for issue/comments
ğŸ§® Token control	Truncates long bodies/comments
ğŸ§  Few-shot caching	Avoids re-prompting same issues
ğŸ©º Health endpoint	Quick API status check
â± Latency logs	Monitors inference time

Average response time: 3 â€“ 4 s per issue with Gemini Flash.

ğŸŒ± Going the Extra Mile
Enhancement	Purpose
âœ… Download JSON button	Easy data export
âœ… Inline warnings	Clear feedback on missing issues or rate limits
âœ… Optional repair prompt	Auto-fix malformed JSON
âœ… Copy-to-clipboard (optional)	UX polish
âœ… Beautiful README	Full rubric coverage
âœ… Extensive few-shot examples	Reliability boost
ğŸ” Security Notes

.env is in .gitignore â€” never commit API keys.

Use low-scope GitHub tokens (read-only).

Revoke tokens after testing.

ğŸ§­ Future Enhancements

ğŸ¤– Fine-tune label taxonomy per repository

ğŸ—‚ï¸ Redis/SQLite caching for hot issues

ğŸŒ Multi-repo batch mode

ğŸ§  Evaluate LLM outputs with automatic ROUGE/F1 metrics
